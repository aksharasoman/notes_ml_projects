{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot using Gemini API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 0: Configure API key\n",
    "\n",
    "The Python SDK for the Gemini API is contained in the google-generativeai package. \n",
    "\n",
    "Install dependency using:\n",
    "*pip install -q -U google-generativeai\n",
    "*\n",
    "\n",
    "Do not check an API key into your version control system but assign it as an environment variable instead:\n",
    "export API_KEY=<YOUR_API_KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-generativeai\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "# os.environ[\"GRPC_VERBOSITY\"] = \"NONE\"\n",
    "\n",
    "# genai.configure(api_key=os.environ[\"API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use system instructions to steer the behavior of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_instr = \"\"\"You are OrderBot, an automated service to collect orders for a pizza restaurant. \n",
    "You first greet the customer, then collects the order,\n",
    "and then asks if it's a pickup or delivery.\n",
    "You wait to collect the entire order, then summarize it and check for a final \n",
    "time if the customer wants to add anything else. \n",
    "If it's a delivery, you ask for an address. \n",
    "Finally you collect the payment.\n",
    "Make sure to clarify all options, extras and sizes to uniquely \n",
    "identify the item from the menu.\n",
    "You respond in a short, very conversational friendly style. \n",
    "The menu includes \n",
    "pepperoni pizza  12.95, 10.00, 7.00 \n",
    "cheese pizza   10.95, 9.25, 6.50 \n",
    "eggplant pizza   11.95, 9.75, 6.75 \n",
    "fries 4.50, 3.50 \n",
    "greek salad 7.25 \n",
    "Toppings: \n",
    "extra cheese 2.00, \n",
    "mushrooms 1.50 \n",
    "sausage 3.00 \n",
    "canadian bacon 3.50 \n",
    "AI sauce 1.50 \n",
    "peppers 1.00 \n",
    "Drinks: \n",
    "coke 3.00, 2.00, 1.00 \n",
    "sprite 3.00, 2.00, 1.00 \n",
    "bottled water 5.00 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  system_instruction=sys_instr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Receive Prompts and Save Context and Generate chat responses\n",
    "*I searched for gemini api help docs.* \n",
    "\n",
    "**ChatSession** class of gemini enables us to have freeform conversation over multiple turns. We dont have to store conversation history as a list.\n",
    "\n",
    "Example: [Build an interactive chat](https://ai.google.dev/gemini-api/docs/text-generation?lang=python#chat)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ChatSession.send_message** method returns the same *GenerateContentResponse* type as **GenerativeModel.generate_content**. It also appends your message and the response to the chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outline of chat using gemini api\n",
    "```\n",
    "model=genai.GenerativeModel( \\\n",
    "  model_name=\"gemini-1.5-flash\") \\\n",
    "chat = model.start_chat( \\\n",
    "    history=[ \\\n",
    "        {\"role\": \"user\", \"parts\": \"Hello\"}, \n",
    "        {\"role\": \"model\", \"parts\": \"Great to meet you. What would you like to know?\"},\n",
    "    ]\n",
    ")\n",
    "response = chat.send_message(\"I have 2 dogs in my house.\")\n",
    "print(response.text)\n",
    "response = chat.send_message(\"How many paws are in my house?\")\n",
    "print(response.text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 2.1 Initialize the chat\n",
    "chat = model.start_chat(history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_messages(_):\n",
    "    prompt = inp.value_input\n",
    "    inp.value = ''\n",
    "    context.append({'role':'user', 'content':f\"{prompt}\"})\n",
    "    response = chat.send_message(context) \n",
    "    context.append({'role':'model', 'parts':f\"{response}\"})\n",
    "    panels.append(\n",
    "        pn.Row('user:', pn.pane.Markdown(prompt, width=600)))\n",
    "    panels.append(\n",
    "        pn.Row('model:', pn.pane.Markdown(response, width=600, style={'background-color': '#F6F6F6'})))\n",
    " \n",
    "    return pn.Column(*panels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Task 3: Build GUI \n",
    "\n",
    "import panel as pn\n",
    "pn.extension() #initialization\n",
    "\n",
    "panels = [] # collect display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = pn.widgets.TextInput(value=\"Hi\", placeholder='Enter text here…')\n",
    "button_conversation = pn.widgets.Button(name=\"Chat!\")\n",
    "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
    "dashboard = pn.Column(\n",
    "    inp,\n",
    "    pn.Row(button_conversation),\n",
    "    pn.panel(interactive_conversation, loading_indicator=True, height=300),\n",
    ")\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_chat('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat(value):\n",
    "    if not value:\n",
    "        value = chat.history\n",
    "    response = chat.send_message(value)\n",
    "    if response:\n",
    "        return f\"Submitted: {response.text}\"\n",
    "    else: \n",
    "        return f\"Submitted: \"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def gemini_callback(contents, user, instance):\n",
    "    # response = await openai.ChatCompletion.acreate(\n",
    "    #     model=\"gpt-3.5-turbo\",\n",
    "    #     messages=[{\"role\": \"user\", \"content\": contents}],\n",
    "    #     stream=True,\n",
    "    # )\n",
    "    response = await chat.send_message(contents)\n",
    "    \n",
    "    message = \"\"\n",
    "    async for chunk in response:\n",
    "        message += chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
    "        yield message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input: 'content' argument must not be empty. Please provide a non-empty value.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m chat_area_input \u001b[38;5;241m=\u001b[39m pn\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mChatAreaInput(placeholder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType something, and press Enter to submit!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m output_markdown \u001b[38;5;241m=\u001b[39m pn\u001b[38;5;241m.\u001b[39mbind(chat\u001b[38;5;241m.\u001b[39msend_message, chat_area_input\u001b[38;5;241m.\u001b[39mparam\u001b[38;5;241m.\u001b[39mvalue_input)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mpn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_area_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_markdown\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2.Projects/obsidian_ml_projects/002_chatbot/codes/.venv/lib/python3.12/site-packages/panel/layout/base.py:819\u001b[0m, in \u001b[0;36mListPanel.__init__\u001b[0;34m(self, *objects, **params)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjects\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms objects should be supplied either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas positional arguments or as a keyword, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    818\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot both.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m--> 819\u001b[0m     params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjects\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[43mpanel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpane\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m pane \u001b[38;5;129;01min\u001b[39;00m objects]\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjects\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    821\u001b[0m     objects \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjects\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/2.Projects/obsidian_ml_projects/002_chatbot/codes/.venv/lib/python3.12/site-packages/panel/pane/base.py:86\u001b[0m, in \u001b[0;36mpanel\u001b[0;34m(obj, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m pane \u001b[38;5;241m=\u001b[39m \u001b[43mPaneBase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pane_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pane\u001b[38;5;241m.\u001b[39mlayout) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pane\u001b[38;5;241m.\u001b[39m_unpack:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pane\u001b[38;5;241m.\u001b[39mlayout[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/2.Projects/obsidian_ml_projects/002_chatbot/codes/.venv/lib/python3.12/site-packages/panel/param.py:831\u001b[0m, in \u001b[0;36mParamRef.__init__\u001b[0;34m(self, object, **params)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_object()\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefer_load:\n\u001b[0;32m--> 831\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_replace_pane\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2.Projects/obsidian_ml_projects/002_chatbot/codes/.venv/lib/python3.12/site-packages/panel/param.py:899\u001b[0m, in \u001b[0;36mParamRef._replace_pane\u001b[0;34m(self, force, *args)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m         new_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_object \u001b[38;5;129;01mis\u001b[39;00m Skip \u001b[38;5;129;01mand\u001b[39;00m new_object \u001b[38;5;129;01mis\u001b[39;00m Undefined:\n\u001b[1;32m    901\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_layout\u001b[38;5;241m.\u001b[39mloading \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/2.Projects/obsidian_ml_projects/002_chatbot/codes/.venv/lib/python3.12/site-packages/panel/param.py:1114\u001b[0m, in \u001b[0;36mParamFunction.eval\u001b[0;34m(self, ref)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval\u001b[39m(\u001b[38;5;28mself\u001b[39m, ref):\n\u001b[0;32m-> 1114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meval_function_with_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2.Projects/obsidian_ml_projects/002_chatbot/codes/.venv/lib/python3.12/site-packages/param/parameterized.py:165\u001b[0m, in \u001b[0;36meval_function_with_deps\u001b[0;34m(function)\u001b[0m\n\u001b[1;32m    163\u001b[0m         args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mgetattr\u001b[39m(dep\u001b[38;5;241m.\u001b[39mowner, dep\u001b[38;5;241m.\u001b[39mname) \u001b[38;5;28;01mfor\u001b[39;00m dep \u001b[38;5;129;01min\u001b[39;00m arg_deps)\n\u001b[1;32m    164\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {n: \u001b[38;5;28mgetattr\u001b[39m(dep\u001b[38;5;241m.\u001b[39mowner, dep\u001b[38;5;241m.\u001b[39mname) \u001b[38;5;28;01mfor\u001b[39;00m n, dep \u001b[38;5;129;01min\u001b[39;00m kw_deps\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2.Projects/obsidian_ml_projects/002_chatbot/codes/.venv/lib/python3.12/site-packages/param/depends.py:53\u001b[0m, in \u001b[0;36mdepends.<locals>._depends\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_depends\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2.Projects/obsidian_ml_projects/002_chatbot/codes/.venv/lib/python3.12/site-packages/param/reactive.py:594\u001b[0m, in \u001b[0;36mbind.<locals>.wrapped\u001b[0;34m(*wargs, **wkwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;129m@depends\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdependencies, watch\u001b[38;5;241m=\u001b[39mwatch)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39mwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mwkwargs):\n\u001b[1;32m    593\u001b[0m     combined_args, combined_kwargs \u001b[38;5;241m=\u001b[39m combine_arguments(wargs, wkwargs)\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meval_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcombined_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcombined_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2.Projects/obsidian_ml_projects/002_chatbot/codes/.venv/lib/python3.12/site-packages/google/generativeai/generative_models.py:564\u001b[0m, in \u001b[0;36mChatSession.send_message\u001b[0;34m(self, content, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported configuration: The `google.generativeai` SDK currently does not support the combination of `stream=True` and `enable_automatic_function_calling=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m     )\n\u001b[1;32m    562\u001b[0m tools_lib \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_get_tools_lib(tools)\n\u001b[0;32m--> 564\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[43mcontent_types\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m content\u001b[38;5;241m.\u001b[39mrole:\n\u001b[1;32m    567\u001b[0m     content\u001b[38;5;241m.\u001b[39mrole \u001b[38;5;241m=\u001b[39m _USER_ROLE\n",
      "File \u001b[0;32m~/2.Projects/obsidian_ml_projects/002_chatbot/codes/.venv/lib/python3.12/site-packages/google/generativeai/types/content_types.py:248\u001b[0m, in \u001b[0;36mto_content\u001b[0;34m(content)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_content\u001b[39m(content: ContentType):\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m content:\n\u001b[0;32m--> 248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument must not be empty. Please provide a non-empty value.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, Mapping):\n\u001b[1;32m    253\u001b[0m         content \u001b[38;5;241m=\u001b[39m _convert_dict(content)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid input: 'content' argument must not be empty. Please provide a non-empty value."
     ]
    }
   ],
   "source": [
    "def output(value):\n",
    "    return f\"Submitted: {value}\"\n",
    "\n",
    "chat_area_input = pn.chat.ChatAreaInput(placeholder=\"Type something, and press Enter to submit!\")\n",
    "output_markdown = pn.bind(chat.send_message, chat_area_input.param.value_input)\n",
    "pn.Row(chat_area_input, output_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_feed = pn.chat.ChatFeed(callback=run_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285d48de65c54ad49b549995c1c1b06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BokehModel(combine_events=True, render_bundle={'docs_json': {'8d93efaf-b366-4c1e-9c77-d311f12ee517': {'version…"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_feed.send()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
