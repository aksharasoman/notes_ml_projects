{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot using Gemini API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 0: Configure API key\n",
    "\n",
    "The Python SDK for the Gemini API is contained in the google-generativeai package. \n",
    "\n",
    "Install dependency using:\n",
    "*pip install -q -U google-generativeai\n",
    "*\n",
    "\n",
    "Do not check an API key into your version control system but assign it as an environment variable instead:\n",
    "export API_KEY=<YOUR_API_KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-generativeai\n",
    "import google.generativeai as genai\n",
    "\n",
    "with open('gemini_api_key.txt','r') as file:\n",
    "    API_KEY = file.read()\n",
    "genai.configure(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use system instructions to steer the behavior of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a context manager to open the file\n",
    "with open('pizzabot_system_instruction.txt', 'r') as file:\n",
    "    sys_instr = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  system_instruction=sys_instr)\n",
    "\n",
    "##### 2.1 Initialize the chat\n",
    "chat = model.start_chat(history=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Receive Prompts and Save Context and Generate chat responses\n",
    "*I searched for gemini api help docs.* \n",
    "\n",
    "**ChatSession** class of gemini enables us to have freeform conversation over multiple turns. We dont have to store conversation history as a list.\n",
    "\n",
    "Example: [Build an interactive chat](https://ai.google.dev/gemini-api/docs/text-generation?lang=python#chat)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ChatSession.send_message** method returns the same *GenerateContentResponse* type as **GenerativeModel.generate_content**. It also appends your message and the response to the chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outline of chat using gemini api\n",
    "```\n",
    "model=genai.GenerativeModel( \\\n",
    "  model_name=\"gemini-1.5-flash\") \\\n",
    "chat = model.start_chat( \\\n",
    "    history=[ \\\n",
    "        {\"role\": \"user\", \"parts\": \"Hello\"}, \n",
    "        {\"role\": \"model\", \"parts\": \"Great to meet you. What would you like to know?\"},\n",
    "    ]\n",
    ")\n",
    "response = chat.send_message(\"I have 2 dogs in my house.\")\n",
    "print(response.text)\n",
    "response = chat.send_message(\"How many paws are in my house?\")\n",
    "print(response.text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_messages(_):\n",
    "    prompt = inp.value_input\n",
    "    inp.value = ''\n",
    "    context.append({'role':'user', 'content':f\"{prompt}\"})\n",
    "    response = chat.send_message(context) \n",
    "    context.append({'role':'model', 'parts':f\"{response}\"})\n",
    "    panels.append(\n",
    "        pn.Row('user:', pn.pane.Markdown(prompt, width=600)))\n",
    "    panels.append(\n",
    "        pn.Row('model:', pn.pane.Markdown(response, width=600, style={'background-color': '#F6F6F6'})))\n",
    " \n",
    "    return pn.Column(*panels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Task 3: Build GUI \n",
    "import panel as pn\n",
    "pn.extension() #initialization\n",
    "\n",
    "panels = [] # collect display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = pn.widgets.TextInput(value=\"Hi\", placeholder='Enter text here‚Ä¶')\n",
    "button_conversation = pn.widgets.Button(name=\"Chat!\")\n",
    "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
    "dashboard = pn.Column(\n",
    "    inp,\n",
    "    pn.Row(button_conversation),\n",
    "    pn.panel(interactive_conversation, loading_indicator=True, height=300),\n",
    ")\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat(value):\n",
    "    if not value:\n",
    "        value = chat.history\n",
    "    response = chat.send_message(value)\n",
    "    if response:\n",
    "        return f\"Submitted: {response.text}\"\n",
    "    else: \n",
    "        return f\"Submitted: \"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def gemini_callback(contents, user, instance):\n",
    "    # response = await openai.ChatCompletion.acreate(\n",
    "    #     model=\"gpt-3.5-turbo\",\n",
    "    #     messages=[{\"role\": \"user\", \"content\": contents}],\n",
    "    #     stream=True,\n",
    "    # )\n",
    "    response = await chat.send_message(contents)\n",
    "    \n",
    "    message = \"\"\n",
    "    async for chunk in response:\n",
    "        message += chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
    "        yield message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat(value):\n",
    "    print(chat)\n",
    "    print(value)\n",
    "    response = chat.send_message(value)\n",
    "    return f\"Assitant: {response.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatSession(\n",
      "    model=genai.GenerativeModel(\n",
      "        model_name='models/gemini-1.5-flash',\n",
      "        generation_config={},\n",
      "        safety_settings={},\n",
      "        tools=None,\n",
      "        system_instruction=\"You are OrderBot, an automated service to collect orders for a pizza restaurant. \\nYou first greet the customer, then collects the order,\\nand then asks if it's a pickup or delivery.\\nYou wait to collect the entire order, then summarize it and check for a final \\ntime if the customer wants to add anything else. \\nIf it's a delivery, you ask for an address. \\nFinally you collect the payment.\\nMake sure to clarify all options, extras and sizes to uniquely \\nidentify the item from the menu.\\nYou respond in a short, very conversational friendly style. \\nThe menu includes \\npepperoni pizza  12.95, 10.00, 7.00 \\ncheese pizza   10.95, 9.25, 6.50 \\neggplant pizza   11.95, 9.75, 6.75 \\nfries 4.50, 3.50 \\ngreek salad 7.25 \\nToppings: \\nextra cheese 2.00, \\nmushrooms 1.50 \\nsausage 3.00 \\ncanadian bacon 3.50 \\nAI sauce 1.50 \\npeppers 1.00 \\nDrinks: \\ncoke 3.00, 2.00, 1.00 \\nsprite 3.00, 2.00, 1.00 \\nbottled water 5.00\",\n",
      "        cached_content=None\n",
      "    ),\n",
      "    history=[protos.Content({'parts': [{'text': 'dog is'}], 'role': 'user'}), protos.Content({'parts': [{'text': 'Hey there!  ... today?  üçï \\n'}], 'role': 'model'}), protos.Content({'parts': [{'text': 'cappucino'}], 'role': 'user'}), protos.Content({'parts': [{'text': \"We don't hav... prefer. üòä \\n\"}], 'role': 'model'})]\n",
      ")\n",
      "Hi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6fc6f2f5804dd19e08887cd7565fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BokehModel(combine_events=True, render_bundle={'docs_json': {'3c8e4a6f-5879-4dd3-87d3-345f39ce616f': {'version‚Ä¶"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatSession(\n",
      "    model=genai.GenerativeModel(\n",
      "        model_name='models/gemini-1.5-flash',\n",
      "        generation_config={},\n",
      "        safety_settings={},\n",
      "        tools=None,\n",
      "        system_instruction=\"You are OrderBot, an automated service to collect orders for a pizza restaurant. \\nYou first greet the customer, then collects the order,\\nand then asks if it's a pickup or delivery.\\nYou wait to collect the entire order, then summarize it and check for a final \\ntime if the customer wants to add anything else. \\nIf it's a delivery, you ask for an address. \\nFinally you collect the payment.\\nMake sure to clarify all options, extras and sizes to uniquely \\nidentify the item from the menu.\\nYou respond in a short, very conversational friendly style. \\nThe menu includes \\npepperoni pizza  12.95, 10.00, 7.00 \\ncheese pizza   10.95, 9.25, 6.50 \\neggplant pizza   11.95, 9.75, 6.75 \\nfries 4.50, 3.50 \\ngreek salad 7.25 \\nToppings: \\nextra cheese 2.00, \\nmushrooms 1.50 \\nsausage 3.00 \\ncanadian bacon 3.50 \\nAI sauce 1.50 \\npeppers 1.00 \\nDrinks: \\ncoke 3.00, 2.00, 1.00 \\nsprite 3.00, 2.00, 1.00 \\nbottled water 5.00\",\n",
      "        cached_content=None\n",
      "    ),\n",
      "    history=[protos.Content({'parts': [{'text': 'dog is'}], 'role': 'user'}), protos.Content({'parts': [{'text': 'Hey there!  ... today?  üçï \\n'}], 'role': 'model'}), protos.Content({'parts': [{'text': 'cappucino'}], 'role': 'user'}), protos.Content({'parts': [{'text': \"We don't hav... prefer. üòä \\n\"}], 'role': 'model'}), protos.Content({'parts': [{'text': 'Hi'}], 'role': 'user'}), protos.Content({'parts': [{'text': 'Hey there! W... today?  üçï \\n'}], 'role': 'model'})]\n",
      ")\n",
      "Cappucino\n",
      "ChatSession(\n",
      "    model=genai.GenerativeModel(\n",
      "        model_name='models/gemini-1.5-flash',\n",
      "        generation_config={},\n",
      "        safety_settings={},\n",
      "        tools=None,\n",
      "        system_instruction=\"You are OrderBot, an automated service to collect orders for a pizza restaurant. \\nYou first greet the customer, then collects the order,\\nand then asks if it's a pickup or delivery.\\nYou wait to collect the entire order, then summarize it and check for a final \\ntime if the customer wants to add anything else. \\nIf it's a delivery, you ask for an address. \\nFinally you collect the payment.\\nMake sure to clarify all options, extras and sizes to uniquely \\nidentify the item from the menu.\\nYou respond in a short, very conversational friendly style. \\nThe menu includes \\npepperoni pizza  12.95, 10.00, 7.00 \\ncheese pizza   10.95, 9.25, 6.50 \\neggplant pizza   11.95, 9.75, 6.75 \\nfries 4.50, 3.50 \\ngreek salad 7.25 \\nToppings: \\nextra cheese 2.00, \\nmushrooms 1.50 \\nsausage 3.00 \\ncanadian bacon 3.50 \\nAI sauce 1.50 \\npeppers 1.00 \\nDrinks: \\ncoke 3.00, 2.00, 1.00 \\nsprite 3.00, 2.00, 1.00 \\nbottled water 5.00\",\n",
      "        cached_content=None\n",
      "    ),\n",
      "    history=[protos.Content({'parts': [{'text': 'dog is'}], 'role': 'user'}), protos.Content({'parts': [{'text': 'Hey there!  ... today?  üçï \\n'}], 'role': 'model'}), protos.Content({'parts': [{'text': 'cappucino'}], 'role': 'user'}), protos.Content({'parts': [{'text': \"We don't hav... prefer. üòä \\n\"}], 'role': 'model'}), protos.Content({'parts': [{'text': 'Hi'}], 'role': 'user'}), protos.Content({'parts': [{'text': 'Hey there! W... today?  üçï \\n'}], 'role': 'model'}), protos.Content({'parts': [{'text': 'Cappucino'}], 'role': 'user'}), protos.Content({'parts': [{'text': \"Sorry, we do... prefer. üòä \\n\"}], 'role': 'model'})]\n",
      ")\n",
      "Coke\n",
      "ChatSession(\n",
      "    model=genai.GenerativeModel(\n",
      "        model_name='models/gemini-1.5-flash',\n",
      "        generation_config={},\n",
      "        safety_settings={},\n",
      "        tools=None,\n",
      "        system_instruction=\"You are OrderBot, an automated service to collect orders for a pizza restaurant. \\nYou first greet the customer, then collects the order,\\nand then asks if it's a pickup or delivery.\\nYou wait to collect the entire order, then summarize it and check for a final \\ntime if the customer wants to add anything else. \\nIf it's a delivery, you ask for an address. \\nFinally you collect the payment.\\nMake sure to clarify all options, extras and sizes to uniquely \\nidentify the item from the menu.\\nYou respond in a short, very conversational friendly style. \\nThe menu includes \\npepperoni pizza  12.95, 10.00, 7.00 \\ncheese pizza   10.95, 9.25, 6.50 \\neggplant pizza   11.95, 9.75, 6.75 \\nfries 4.50, 3.50 \\ngreek salad 7.25 \\nToppings: \\nextra cheese 2.00, \\nmushrooms 1.50 \\nsausage 3.00 \\ncanadian bacon 3.50 \\nAI sauce 1.50 \\npeppers 1.00 \\nDrinks: \\ncoke 3.00, 2.00, 1.00 \\nsprite 3.00, 2.00, 1.00 \\nbottled water 5.00\",\n",
      "        cached_content=None\n",
      "    ),\n",
      "    history=[protos.Content({'parts': [{'text': 'dog is'}], 'role': 'user'}), protos.Content({'parts': [{'text': 'Hey there!  ... today?  üçï \\n'}], 'role': 'model'}), protos.Content({'parts': [{'text': 'cappucino'}], 'role': 'user'}), protos.Content({'parts': [{'text': \"We don't hav... prefer. üòä \\n\"}], 'role': 'model'}), protos.Content({'parts': [{'text': 'Hi'}], 'role': 'user'}), protos.Content({'parts': [{'text': 'Hey there! W... today?  üçï \\n'}], 'role': 'model'}), protos.Content({'parts': [{'text': 'Cappucino'}], 'role': 'user'}), protos.Content({'parts': [{'text': \"Sorry, we do... prefer. üòä \\n\"}], 'role': 'model'}), protos.Content({'parts': [{'text': 'Coke'}], 'role': 'user'}), protos.Content({'parts': [{'text': 'Okay, one co...and large. \\n'}], 'role': 'model'})]\n",
      ")\n",
      "medium\n"
     ]
    }
   ],
   "source": [
    "def output(value):\n",
    "    print(chat)\n",
    "    response = chat.send_message(value)\n",
    "    return f\"Assitant: {response.text}\"\n",
    "\n",
    "chat_area_input = pn.chat.ChatAreaInput(value=\"Hi\",placeholder=\"Type something, and press Enter to submit!\")\n",
    "output_markdown = pn.bind(run_chat, chat_area_input.param.value)\n",
    "pn.Column(chat_area_input, output_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
