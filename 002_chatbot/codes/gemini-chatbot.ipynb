{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot using Gemini API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 0: Configure API key\n",
    "\n",
    "The Python SDK for the Gemini API is contained in the google-generativeai package. \n",
    "\n",
    "Install dependency using:\n",
    "*pip install -q -U google-generativeai\n",
    "*\n",
    "\n",
    "Do not check an API key into your version control system but assign it as an environment variable instead:\n",
    "export API_KEY=<YOUR_API_KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-generativeai\n",
    "import google.generativeai as genai\n",
    "\n",
    "with open('gemini_api_key.txt','r') as file:\n",
    "    API_KEY = file.read()\n",
    "genai.configure(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use system instructions to steer the behavior of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a context manager to open the file\n",
    "with open('pizzabot_system_instruction.txt', 'r') as file:\n",
    "    sys_instr = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  system_instruction=sys_instr)\n",
    "\n",
    "##### 2.1 Initialize the chat\n",
    "chat = model.start_chat(history=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Receive Prompts and Save Context and Generate chat responses\n",
    "*I searched for gemini api help docs.* \n",
    "\n",
    "**ChatSession** class of gemini enables us to have freeform conversation over multiple turns. We dont have to store conversation history as a list.\n",
    "\n",
    "Example: [Build an interactive chat](https://ai.google.dev/gemini-api/docs/text-generation?lang=python#chat)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ChatSession.send_message** method returns the same *GenerateContentResponse* type as **GenerativeModel.generate_content**. It also appends your message and the response to the chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outline of chat using gemini api\n",
    "```\n",
    "model=genai.GenerativeModel( \\\n",
    "  model_name=\"gemini-1.5-flash\") \\\n",
    "chat = model.start_chat( \\\n",
    "    history=[ \\\n",
    "        {\"role\": \"user\", \"parts\": \"Hello\"}, \n",
    "        {\"role\": \"model\", \"parts\": \"Great to meet you. What would you like to know?\"},\n",
    "    ]\n",
    ")\n",
    "response = chat.send_message(\"I have 2 dogs in my house.\")\n",
    "print(response.text)\n",
    "response = chat.send_message(\"How many paws are in my house?\")\n",
    "print(response.text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_messages(_):\n",
    "    prompt = inp.value_input\n",
    "    inp.value = ''\n",
    "    context.append({'role':'user', 'content':f\"{prompt}\"})\n",
    "    response = chat.send_message(context) \n",
    "    context.append({'role':'model', 'parts':f\"{response}\"})\n",
    "    panels.append(\n",
    "        pn.Row('user:', pn.pane.Markdown(prompt, width=600)))\n",
    "    panels.append(\n",
    "        pn.Row('model:', pn.pane.Markdown(response, width=600, style={'background-color': '#F6F6F6'})))\n",
    " \n",
    "    return pn.Column(*panels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Task 3: Build GUI \n",
    "import panel as pn\n",
    "pn.extension() #initialization\n",
    "\n",
    "panels = [] # collect display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = pn.widgets.TextInput(value=\"Hi\", placeholder='Enter text here…')\n",
    "button_conversation = pn.widgets.Button(name=\"Chat!\")\n",
    "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
    "dashboard = pn.Column(\n",
    "    inp,\n",
    "    pn.Row(button_conversation),\n",
    "    pn.panel(interactive_conversation, loading_indicator=True, height=300),\n",
    ")\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat(value):\n",
    "    if not value:\n",
    "        value = chat.history\n",
    "    response = chat.send_message(value)\n",
    "    if response:\n",
    "        return f\"Submitted: {response.text}\"\n",
    "    else: \n",
    "        return f\"Submitted: \"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def gemini_callback(contents, user, instance):\n",
    "    # response = await openai.ChatCompletion.acreate(\n",
    "    #     model=\"gpt-3.5-turbo\",\n",
    "    #     messages=[{\"role\": \"user\", \"content\": contents}],\n",
    "    #     stream=True,\n",
    "    # )\n",
    "    response = await chat.send_message(contents)\n",
    "    \n",
    "    message = \"\"\n",
    "    async for chunk in response:\n",
    "        message += chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
    "        yield message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat(value):\n",
    "    response = chat.send_message(value)\n",
    "    return f\"Assitant: {response.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efab82b716ec4f9ebbaabe5fc056fc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BokehModel(combine_events=True, render_bundle={'docs_json': {'fc57d171-3cd5-4eb7-8e23-dcaa529e5f3e': {'version…"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_area_input = pn.chat.ChatAreaInput(value=\"Hi\",placeholder=\"Type something, and press Enter to submit!\")\n",
    "output_markdown = pn.bind(run_chat, chat_area_input.param.value)\n",
    "pn.Column(chat_area_input, output_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
